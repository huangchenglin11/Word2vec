{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a039c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4121206",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa958a1-c47a-402d-a21a-09c712b93e45",
   "metadata": {},
   "source": [
    "## 数据集 Dataset\n",
    "### 1、添加一个Markdown单元格，在其中解释下方单元格的两行代码。\n",
    "#### 设置 os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' ，这样做具体改变了什么？ 为什么要设置HF_ENDPOINT='https://hf-mirror.com'而非直接使用官方源？ dataset = datasets.load_dataset(\"bentrevett/multi30k\") 这行代码具体完成了什么操作？\n",
    "\n",
    "#### 1、这行代码设置了环境变量HF_ENDPOINT，将Hugging Face的默认数据/模型下载地址从官方源(https://huggingface.co)改为镜像源(https://hf-mirror.com)。 后续通过datasets或transformers库发起的下载请求会指向该镜像站点。 2、为什么使用镜像而非官方源： 官方源(huggingface.co)可能在某些地区（如中国大陆）访问较慢或不稳定，镜像源通常部署在本地或优化线路的服务器上，能显著提升下载速度。 官方源对高频请求可能限流，镜像源可分担压力。某些场景下需通过合规镜像访问数据（如企业内网）。 二、dataset = datasets.load_dataset(\"bentrevett/multi30k\")这行代码具体完成了什么操作。 1、从Hugging Face下载一个叫multi30k的数据集（由用户 bentrevett 上传）。 如果之前下载过，就直接用本地缓存的版本（不用重复下载）。 如果没下载过，就自动从Hugging Face官网（或指定的镜像站）下载。 2、返回一个可以直接用的数据集对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64395c0b-27c3-4bb4-beb7-ea27932452d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since bentrevett/multi30k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\bentrevett___multi30k\\default\\0.0.0\\4589883f3d09d4ef6361784e03f0ead219836469 (last modified on Mon May 19 09:39:05 2025).\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a421bc-cb94-4b56-8be6-4fc93154e827",
   "metadata": {},
   "source": [
    "### 2、运行下方的单元格。\n",
    "#### 你会看到数据集对象（一个DatasetDict）包含训练、验证和测试集，每个集合中的样本数量，以及每个集合中的特征（“en”和“de”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7112907-b642-4e12-9f1d-64afb79c82c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30da4141-6d5f-419a-a053-fd77dd9c2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fd2a6-b34f-415f-b97e-fc5cdac30800",
   "metadata": {},
   "source": [
    "### 3、运行下方的单元格。\n",
    "#### 我们可以索引每个数据集来查看单个示例。每个例子都有两个特征：“en”和“de”，是对应的英语和德语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4da5f62-ca4f-4e10-910b-5cd508f2d93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe34fba-8114-411c-a4b8-96ec64fcfff9",
   "metadata": {},
   "source": [
    "#### 接下来我们进行分词。英语/德语的分词较中文要直接，比如句子\"good morning!会被分词为[\"good\", \"morning\", \"!\"]序列。  下方的代码要成功安装en_core_web_sm和de_core_news_sm后才不会报错。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d8118-568d-4d14-bbb0-1ccc383dd121",
   "metadata": {},
   "source": [
    "## 分词器 Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2548621f-053e-4095-89ba-d8de4c23caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9bd10-db30-4502-90d7-7fa6c082d313",
   "metadata": {},
   "source": [
    "### 4、运行下方的单元格。\n",
    "#### 我们可以使用.tokenizer方法调用每个spaCy模型的分词器，该方法接受字符串并返回Token对象序列。我们可以使用text属性从Token对象中获取字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3122003a-8ecc-40da-bfe9-67be6673f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"What a lovely day it is today!\"\n",
    "\n",
    "[token.text for token in en_nlp.tokenizer(string)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cdc0b-a52a-4e8b-aadf-f993dbd305d5",
   "metadata": {},
   "source": [
    "### 5、添加一个Markdown单元格，在其中解释下方单元格的函数的作用。\n",
    "#### 函数功能： 英语分词： 使用en_nlp.tokenizer对example[\"en\"]（英语句子）分词，生成一个token列表。 截取前max_length个token（避免过长）。 德语分词： 使用de_nlp.tokenizer对example[\"de\"]（德语句子）分词，生成一个token列表。 截取前max_length个token。 小写处理（可选）： 如果lower=True，将英语和德语的所有token转为小写。 添加特殊标记： 在英语和德语的token列表的开头插入sos_token（句子起始标记）。 在末尾插入eos_token（句子结束标记）。 返回结果： 返回一个字典，包含处理后的英语和德语token列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bb8c57-d1ab-47f5-8f12-9b25562f484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9074b48-ad6f-42d2-b47a-374fd9f7347d",
   "metadata": {},
   "source": [
    "### 6、添加一个Markdown单元格，在其中解释下方单元格出现的<sos>和<eos>的含义，以及map函数的作用。\n",
    "#### 和的含义: （Start of Sentence，句子起始标记）。 表示一个句子的开始，通常用于告诉模型“从这里开始生成/解码”。 在机器翻译任务中，通常作为解码器的第一个输入，帮助模型知道何时开始生成目标语言（如德语）的句子。 （End of Sentence，句子结束标记）。 表示一个句子的结束，告诉模型“生成到这里可以停止了”。 在解码过程中，模型遇到时会停止生成后续token避免无限生成。 map函数的作用 这里的map是数据集（如Hugging Face Dataset或tf.data.Dataset）的方法，作用是对数据集中的每个样本应用tokenize_example函数，实现批量处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a9a4982-1450-4c14-887a-f5ba149e30d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36da682998d4c20b3d175d36390c9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817758d2-20fc-4422-b75d-3b5140b06765",
   "metadata": {},
   "source": [
    "### 7、运行下方的单元格\n",
    "#### 重新打印train_data[0]，验证小写字符串列表以及序列标记的开始/结束符已被成功添加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85a65c02-7e83-4bdb-b365-98676c5dd1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf09ac2-50e2-4190-8e45-87fce6b08b0b",
   "metadata": {},
   "source": [
    "### 词汇表 Vocabularies\n",
    "#### 下一个步骤是为源语言和目标语言构建词汇表，将词语映射为数字索引。比如\"hello\" = 1, \"world\" = 2, \"bye\" = 3, \"hates\" = 4。当向我们的模型提供文本数据时，我们使用词汇表作为look-up-table将字符串转换为标记，然后将标记转换为数字。“hello world”变成了“[“hello”，“world”]”，然后变成了“[1,2]”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c882b3c4-8f54-40ad-bba2-c01c8af07992",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf96490-937d-459b-9b36-4e69a29add94",
   "metadata": {},
   "source": [
    "### 8、运行下方两个单元格\n",
    "#### 验证词汇表，分别打印英语词汇表和德语词汇表的前十个Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2c7a5b8-2e20-43b5-99a6-1f06383f8f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "676188e6-6287-498d-ab38-61d35dd5b45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in', 'eine', ',']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e9dcf-d3ff-4238-9022-58cb091810ad",
   "metadata": {},
   "source": [
    "### 9、运行下方的单元格\n",
    "#### 使用get_stoi（stoi = \"string to int \"）方法获取指定的Token的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a5f9e8-7fce-4b92-a2d4-b0013c9f5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c6f0bb5-4347-4b31-9af9-217cc2c65465",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a2bcfc1-0f99-4cff-a1dc-a1f53bef4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8ea87-a4e1-48e5-8cbe-12086f2f1e54",
   "metadata": {},
   "source": [
    "#### 词汇表的另一个有用特性是lookup_indices方法。它接受一个Token列表并返回一个索引列表。\n",
    "\n",
    "### 10、运行下方的单元格\n",
    "#### 观察从Token列表到索引列表的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade90b10-b2ea-459b-9be9-4c6b1d3802a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[956, 2169, 173, 0, 821]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"i\", \"love\", \"watching\", \"crime\", \"shows\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0236d-daaa-4428-8daa-3033d83c2331",
   "metadata": {},
   "source": [
    "### 12、添加一个Markdown单元格，在其中解释为什么原本的\"crime\"被转换成了<unk>。\n",
    "#### 使用了min_freq=2过滤低频词。只有出现次数 ≥ 2的单词才会被加入词表，否则会被映射为 （未知词）。\n",
    "\n",
    "### 13、添加一个Markdown单元格，在其中解释下方两个单元格中代码的作用。\n",
    "#### 下面两段代码的作用是对数据集中的文本进行数值化处理，将文本转换为对应的索引ID，以便用于机器翻译等自然语言处理任务。 numericalize_example函数： 输入参数：example（包含\"en_tokens\"和\"de_tokens\"字段的样本）、en_vocab（英语词汇表）、de_vocab（德语词汇表）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4436ef-1ef7-4714-ab1a-28413706a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a90a1958-8f03-4a5f-a026-ac9ec1aa9f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feda6279287242969165d263c3ca4bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6de714-289f-4d94-b607-0fc05b824072",
   "metadata": {},
   "source": [
    "### 14、运行下方的单元格\n",
    "#### 重新打印train_data[0]，验证\"en_ids\" and \"de_ids\"被成功添加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e55170c6-45b9-4b05-a065-9a2ac2cac933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'en_ids': [2, 16, 24, 15, 25, 778, 17, 57, 80, 202, 1312, 5, 3],\n",
       " 'de_ids': [2, 18, 26, 253, 30, 84, 20, 88, 7, 15, 110, 7647, 3171, 4, 3]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943eb08-cf39-48bd-98f3-2d3a32cfd5d3",
   "metadata": {},
   "source": [
    "#### Dataset类为我们处理的另一件事是将features转换为正确的类型。每个例子中的索引目前都是基本的Python整数。然而，为了在PyTorch中使用它们，它们需要转换为PyTorch张量。with_format方法将columns参数转换为给定的类型。这里，我们指定类型为“torch”，columns为“en_ids”和“de_ids”（我们想要转换为PyTorch张量的features）。默认情况下，with_format将删除任何不在传递给列的features列表中的features。我们希望保留这些features，这可以通过output_all_columns=True来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1451295d-5bd9-4c64-abc1-61b8499ef06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147109b-9f8a-4192-b979-4589bb8adfa9",
   "metadata": {},
   "source": [
    "### 15、运行下方的单元格\n",
    "#### 重新打印train_data[0]，验证“en_ids”和“de_ids”特征被转换为了张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0406183a-ded3-4cd4-ae11-190cc9653f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([   2,   16,   24,   15,   25,  778,   17,   57,   80,  202, 1312,    5,\n",
       "            3]),\n",
       " 'de_ids': tensor([   2,   18,   26,  253,   30,   84,   20,   88,    7,   15,  110, 7647,\n",
       "         3171,    4,    3]),\n",
       " 'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf02f75-fd2c-4d2e-a0b9-d0ba923c4445",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "#### 数据准备的最后一步是创建Data Loaders。可以对它们进行迭代以返回一批数据，每一批数据都是一个字典，其中包含数字化的英语和德语句子作为PyTorch张量。\n",
    "\n",
    "### 16、添加一个Markdown单元格，在其中解释下方两个单元格中的函数的作用。\n",
    "#### get_collate_fn(pad_index)函数 作用：生成一个自定义的collate_fn函数，用于处理批次数据中的变长序列。 实现细节： 输入的batch是一个列表，每个元素是数据集中的一个样本（包含\"en_ids\"和\"de_ids\"字段）。 batch_en_ids和batch_de_ids分别提取批次中的英语和德语token ID序列。 nn.utils.rnn.pad_sequence对变长序列进行填充： 将批次中所有序列填充到相同长度（最长序列的长度）。 padding_value=pad_index指定填充用的索引（通常是词汇表中的标记）。 返回一个字典，包含填充后的英语和德语序列张量。 2、get_data_loader(dataset, batch_size, pad_index, shuffle=False)函数 作用：创建配置好的DataLoader对象，用于批量加载数据。 参数： dataset为自定义数据集对象（需实现__getitem__和__len__）。 batch_size为每批次的样本数量。 pad_index为填充token的索引。 shuffle设置是否打乱数据（默认为False）。 实现细节： 调用get_collate_fn生成自定义的collate_fn。 通过torch.utils.data.DataLoader初始化数据加载器，并传入自定义的collate_fn。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "121a8aa8-be9e-40c1-b5be-70b2307cdf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89523080-d16b-48d2-8b98-2e19e32cd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ae9273-bfa0-4689-83a9-d81f9ffd8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7717fc3-f508-4937-9290-6d2547d38c2d",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "#### 我们将分三部分构建模型。编码器，解码器和封装编码器和解码器的seq2seq模型。\n",
    "\n",
    "## 编码器 Encoder\n",
    "#### 首先是编码器，它是一个2层的LSTM。\n",
    "\n",
    "### 17、添加一个Markdown单元格，解释下方单元格中Encoder类的代码。\n",
    "#### 包括输入参数，核心组件（词嵌入层、LSTM层、Dropout层），forwad函数的处理流程，和输出。\n",
    "#### 下方class Encoder(nn.Module):\n",
    "\n",
    "#### 输入参数 input_dim：词汇表大小（词表维度）。 embedding_dim：词嵌入的维度（将离散词索引映射为连续向量）。 hidden_dim：LSTM隐藏状态的维度（决定模型容量）。 n_layers：LSTM的层数（堆叠多层以增加模型深度）。 dropout：Dropout概率（防止过拟合）。 核心组件 词嵌入层(nn.Embedding)： 将输入的词索引转换为稠密向量（词嵌入），形状为[src length, batch size, embedding dim]。 LSTM层 (nn.LSTM)： 处理序列数据并生成隐藏状态和细胞状态。支持多层堆叠和Dropout。 Dropout层(nn.Dropout)： 在词嵌入后应用随机失活（训练时生效），防止过拟合。 前向传播 (forward函数) 输入src， 形状为[src length, batch size]，表示一个批次的序列（每个序列已转换为词索引）。 处理流程： 词嵌入：self.embedding(src)将词索引映射为向量。 Dropout：对词嵌入向量随机置零。 LSTM处理： outputs：所有时间步的隐藏状态（未使用），形状为[src length, batch size, hidden dim]。 hidden和cell：最后一层的最终隐藏状态和细胞状态，形状为[n layers, batch size, hidden dim]（用于解码器初始化）。 输出 返回值：(hidden, cell)：用于初始化解码器的初始状态，包含序列的上下文信息。 形状说明： hidden/cell：[n layers, batch size, hidden dim]（单向LSTM）或[n layers * 2, batch size, hidden dim]（双向LSTM）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcceb5cb-f37d-4a58-b317-8350fbad1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52dfbd-3444-40bc-8445-3379d8ec13b2",
   "metadata": {},
   "source": [
    "## 解码器 Decoder\n",
    "#### 接下来是解码器，它需要与编码器对齐，同样是一个2层的LSTM。\n",
    "\n",
    "### 18、添加一个Markdown单元格，描述Decoder的工作流程。\n",
    "#### 下方class Decoder(nn.Module):\n",
    "\n",
    "#### 初始化阶段 参数设置：定义输出维度output_dim、词嵌入维度embedding_dim、隐藏层维度hidden_dim、LSTM层数n_layers和Dropout率。 组件初始化： nn.Embedding：将输入的token ID映射为稠密向量（词嵌入）。 nn.LSTM：处理嵌入后的序列，通过多层LSTM逐步提取特征。 nn.Linear：将LSTM输出的隐藏状态映射到目标词汇表维度。 nn.Dropout：防止过拟合。 前向传播流程 输入处理： 输入形状为[batch_size]，通过unsqueeze(0)扩展为[1, batch_size]，以适配LSTM的序列长度维度。 词嵌入层将输入转换为[1, batch_size, embedding_dim]，并应用Dropout。 LSTM处理： 输入嵌入向量与初始隐藏状态hidden和cell状态传递给LSTM。 LSTM输出： output：当前时间步的隐藏状态，形状为[1, batch_size, hidden_dim]（序列长度和方向数均为1）。 更新后的hidden和cell：形状为[n_layers, batch_size, hidden_dim]，用于下一个时间步。 预测生成： 通过线性层fc_out将LSTM输出的隐藏状态output.squeeze(0)映射到目标词汇表维度[batch_size, output_dim]，生成每个token的概率分布。关键特点 自回归生成：每次处理一个token，依赖前一步的隐藏状态和cell状态（类似掩码机制，避免未来信息泄露）。 序列建模：通过LSTM的循环结构逐步生成序列，适合处理时序依赖任务（如机器翻译）。 与Transformer解码器的区别： 该实现基于RNN，而Transformer解码器使用自注意力和交叉注意力机制。 RNN解码器是串行处理，Transformer可并行训练但需串行预测。 输出与迭代 返回当前时间步的预测结果prediction和更新后的LSTM状态hidden, cell，用于下一步生成或损失计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b30db77e-7d67-4f66-bc95-ba4b529c6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a006e4e-3374-4065-a9cd-577d99ad2029",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "### 19、添加一个Markdown单元格，解释下方单元格中Seq2Seq类的代码。\n",
    "#### 包括forward函数的流程，以及teacher forcing机制。\n",
    "#### class Seq2Seq(nn.Module):初始化__init__ 输入： encoder：编码器（通常为RNN/LSTM/GRU），负责将输入序列编码为固定长度的上下文向量。 decoder：解码器（结构类似编码器），负责基于上下文向量生成目标序列。 device：指定计算设备（CPU/GPU）。 关键检查： 编码器和解码器的隐藏层维度hidden_dim必须相同。 编码器和解码器的层数n_layers必须相同。 前向传播（forward） (1) 编码阶段（Encoder） 输入处理： src：源序列，形状为[src_length, batch_size]。 trg：目标序列，形状为[trg_length, batch_size]。 编码器输出： 编码器处理src，返回最终的隐藏状态hidden和cell状态。这些状态将作为解码器的初始状态。 (2) 解码阶段（Decoder） 初始输入： 解码器的第一个输入是目标序列的起始标记（trg[0, :]）。 循环生成序列： 遍历目标序列的每个时间步t（从1到trg_length-1）： 解码器预测： output是当前时间步的预测结果（词汇表上的概率分布）。 结果存入outputs[t]以便后续计算损失。 Teacher Forcing机制： 以概率teacher_forcing_ratio决定是否使用真实标签（trg[t]）或模型预测（top1 = output.argmax(1)）作为下一个输入： 返回结果： outputs：形状[trg_length, batch_size, trg_vocab_size]，包含每个时间步的预测结果。 Teacher Forcing机制 作用：加速训练并缓解误差累积问题。 训练时：以概率teacher_forcing_ratio使用真实标签作为解码器的输入，避免早期预测错误影响后续生成。 预测时：完全依赖模型自身的预测（无真实标签可用）。 优点：加快收敛，减少训练初期的不稳定性。 缺点：可能导致模型过度依赖真实标签，在预测时表现下降（Exposure Bias问题）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7906316-a475-4249-ac73-619d08d354ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae01a2-0ddf-40d6-bbaa-d94236d716b0",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "#### 模型初始化\n",
    "\n",
    "### 20、添加注释\n",
    "#### 分别将“# 编码器初始化”，“# 解码器初始化”，“# Seq2Seq模型整合”这三行注释加到下方单元格中正确的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4633bd5a-b2ba-4bfa-af9a-5019f31375a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(de_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e688aa-2201-4883-8567-966da0171983",
   "metadata": {},
   "source": [
    "#### 权重初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5185f6f0-a0db-43ed-8094-123fc63487ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ba07f20-ce21-45e5-aceb-795e4f0d5a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,898,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed099bc3-52cd-49f7-ac30-2c845c22c7c7",
   "metadata": {},
   "source": [
    "#### 优化器 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72f3d322-b908-466a-923b-0d9f9251f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81132a-2131-401e-b674-c3a79f92f9e0",
   "metadata": {},
   "source": [
    "#### 损失函数 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dadb41dc-957d-46f6-908e-051c79d5350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a8e11-c873-4415-9b58-f98ae96b2bab",
   "metadata": {},
   "source": [
    "#### Training Loop:\n",
    "\n",
    "### 21、给下方单元格中的代码逐行加注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa8f2e1e-81b1-40d6-8aa4-0064f92002c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"de_ids\"].to(device)\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc198a-7cf5-47bf-90c3-46dae8f8506c",
   "metadata": {},
   "source": [
    "#### Evaluation Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34629533-2133-4055-a121-e0022bc05438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"de_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575da2c5-2e94-458e-a7f6-ed5ab7520fff",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145581c6-926d-4821-9543-fbfd6b1491c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1 # 因模型训练对计算资源要求较高，此处只设立了一轮训练。\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e7523-3f0d-4c96-b589-bb231f0a0ab1",
   "metadata": {},
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e8a09-509f-43cf-8aad-5c561b809354",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"tut1-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c000f62-338d-41b4-9707-a84cd34db78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = de_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48262e-8cb1-4751-8a48-c4fa6aed136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_data[0][\"de\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0b8b2-d14c-4f01-abd3-5ecf953edbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9bbea9-7405-4add-8a78-e1186b2a7829",
   "metadata": {},
   "source": [
    "### 22、运行下方单元格，得到测试集第0个索引的翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bc729-2b1f-413a-830b-b98941fbc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d92a9-e446-4656-9bd7-9811076bd0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
